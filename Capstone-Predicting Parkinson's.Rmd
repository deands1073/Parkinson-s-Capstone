---
title: "Capstone Project - Predicting Parkinson's"
author: "Dean D'Souza"
date: '2024-04-28'
output: pdf_document
---



#All Libraries
```{r}
library(caret)  #modeling
library(tidyverse)
library(ggformula)  #graphing
library(VIM) #missingness
library(corrplot)#corr plots
library(mice) #imputation
library(RColorBrewer) #color
library(Boruta)   #feature selection
library(performanceEstimation) #SMOTE sampling
library(forcats) #Factoring in R
library(car)    #raw correlations
library(glmnet) #penalized regularization
library(pROC)
library(rempsyc)     #APA formatted tables 
library(fastshap)  #Shapley value approximation
library(shapviz)  #Shapley vizualizations
library(gtools)  #function for apa graphs theme
```


###################################################################
##### Data Preperation #####				 
###################################################################

#Reading in Data
PPMI provides a data dictionary along with the curated dataset. Easiest way to read in through read.csv.The function read_excel produces errors when coercing to factors. Changes to excel file would have to be needed. 
```{r}
#PPMI_dict = read_excel("PPMI/PPMI_Curated_Data_Cut_Dictionary_Public_20231214.xlsx")
PPMI = read.csv("PPMI/PPMI_Curated_Data_Cut_Public_20230612_rev.csv", header = TRUE)

```

#Removing unimportant variables or direct diagnosis of the disease
Preliminary column removal. The variable 'study_status' is present in the data dictionary but absent from the curated dataset. It would have been remove regardless. 
```{r}
#List of unrelated variables to predict parkinsons
removedlist = c(
'SITE',
'subgroup',
'PHENOCNV',
'DIAG1',
'DIAG1VIS',
'DIAG2',
'DIAG2VIS',
'DIAG3',
'DIAG3VIS',
'fampd',
'agediag',
'ageonset',
'age_LP',
'age_DATSCAN',
'age_upsit',
'cogstate',
'DOMSIDE',
'duration',
'duration_yrs',
'LEDD',
'OTHNEURO',
'PDTRTMNT',
'PRIMDIAG',
'sym_tremor',
'sym_rigid',
'sym_brady',
'sym_posins',
'sym_other',
'sym_unknown',
'pm_adl_any',
'pm_any',
'pm_auto_any',
'pm_cog_any',
'pm_fd_any',
'pm_mc_any',
'pm_wb_any')


#PPMI.dropUnimportant = subset(PPMI, select = !(names(PPMI) %in% removedlist)) 
PPMI.dropUnimportant = 
  PPMI %>% 
  select(!all_of(removedlist))
```

#Fix Cohort values
From data dictionary:
"For analysis purposes, the Analytic Cohort (CONCOHORT) should be used.  If Analytic Cohort is missing, use COHORT."
```{r}
PPMI.dropUnimportant = PPMI.dropUnimportant %>% 
  mutate(CONCOHORT = ifelse(is.na(CONCOHORT), COHORT, CONCOHORT),
         COHORT = NULL)
  
```

#Find and Convert categorical variables to factor
```{r}
#Find columns where unique values are less than 5
num_unique_values = 5
factor_columns = sapply(PPMI.dropUnimportant, function(column_check) length(unique(column_check)) < num_unique_values)

#change to factor
PPMI.dropUnimportant = PPMI.dropUnimportant %>%
  mutate(across(names(factor_columns)[factor_columns],as.factor))

  
```

#Check encodings
```{r}
str(PPMI.dropUnimportant)
```
#Change specific variables to factor not caught by unique values
```{r}

col_to_factor = c('EVENT_ID', 'visit_date' , 'race', 'quip', 'CSFSAA')
PPMI.dropUnimportant =
PPMI.dropUnimportant %>% 
  mutate(across(col_to_factor, factor)) %>% 
  mutate_if(is.character, as.numeric)# %>% 
  #str(PPMI_only_neccessary)
```

#Dropping UDPRS related variables
Distinction was made for UDPRS for different designs. Study could have focused on only removing UDPRS from predictions.
```{r}
UDPRS_variables = c(
'NP1ANXS',
'NP1APAT' ,
'NP1COG',
'NP1DDS',
'NP1DPRS',
'NP1FATG',
'NP1HALL',
'updrs1_score',
'updrs2_score',
'updrs3_score',
'updrs3_score_on',
'updrs4_score',
'updrs_totscore',
'updrs_totscore_on'
                      )

#PPMI_removed_UDPRS = subset(PPMI_only_neccessary, select = !(names(PPMI_only_neccessary) %in% UDPRS_variables))
PPMI.dropUDPRS.dropUnimportant = 
  PPMI.dropUnimportant %>% 
  select(!all_of(UDPRS_variables))
```

#Drop all other subjective tests
```{r}
All_other_subjective = c(
  'upsit',   #University of Pennsylvania Smell Identification Test
  'upsit_pctl',
  'upsit_pctl15',
  'moca',  #The Montreal Cognitive Assessment
  'bjlot', #Benton Judgement of Line Orientation Score
  'clockdraw',  #Clockdrawing test
  'hvlt_discrimination',  #Hopkins Verbal Learning Test
  'hvlt_immediaterecall',
  'hvlt_retention',
  'HVLTFPRL',
  'HVLTRDLY',
  'HVLTREC',
  'lexical', #Lexical Fluency Score
  'lns', #Letter Number Sequencing Score
  'MODBNT', #Modified Boston Naming Test Score
  'SDMTOTAL', #Symbol Digit Modalities Score
  'TMT_A', #Trail Making Test
  'TMT_B',               
  'SDMTOTAL', #Semantic Fluency Score - Animal subscore
  'MCI_testscores',   #Indication of Mild cognitive impairment based on cognitive test scores (at least two cognitive test scores >1.5 SD below standardized mean)
  'MSEADLG', #Modified Schwab & England ADL Score
  'ess', #Epworth Sleepiness Scale Score
  'rem', #REM Sleep Behavior Disorder Questionnaire Score
  'gds', #Geriatric Depression Scale Score
  'stai',  #STAI Total Score (Anxiety Test)
  'stai_state',
  'stai_trait',
  'scopa',        #Scales for Outcomes in Parkinsonâ€™s disease - Autonomic Dysfunction
  'scopa_cv',     
  'scopa_gi',
  'scopa_pm',
  'scopa_sex',
  'scopa_therm',
  'scopa_ur',
  'hy',  #Reclassified Hoehn & Yahr Stage (includes OFF and untreated scores)
  'hy_on',             
  'NHY',   #Hoehn & Yahr Stage (includes OFF and untreated scores)
  'NHY_ON',                              
  'pigd', #PIGD OFF score (includes OFF and untreated scores
  'pigd_on',
  'td_pigd',
  'td_pigd_on',
  'VLTANIM'
)


PPMI.removeSubjective = 
  PPMI.dropUDPRS.dropUnimportant %>% 
  select(!all_of(All_other_subjective))
```

#Filter on Baseline data only
```{r}
PPMI.filterBL = 
filter(PPMI.removeSubjective, EVENT_ID == 'BL' )
```

#Filter on PD/healthy ( 1/2 )

```{r}
healthy_and_PD = c('1','2') # 1 is PD, 2 is healthy control
PPMI.filterPDHealthy.filterBL =
PPMI.filterBL %>% 
 filter(CONCOHORT %in% healthy_and_PD) 
```

#Change 1 and 2 to "PD" and "Healthy" for clarity
The values are still 1 and 2 but the factor labels are renamed
```{r}
PPMI.filterPDHealthy.filterBL = 
PPMI.filterPDHealthy.filterBL %>% 
  mutate(CONCOHORT = fct_recode(CONCOHORT, 
                                PD = "1",
                                Healthy = "2"))
```

#Check and drop extra factor levels
CONCOHORT has 4 levels from original values. Even after filtering the factor levels remain.
Need to drop levels to keep only PD and healthy.
```{r}
str(PPMI.filterPDHealthy.filterBL)
levels(PPMI.filterPDHealthy.filterBL$CONCOHORT)
PPMI.filterPDHealthy.filterBL = droplevels(PPMI.filterPDHealthy.filterBL)
```

#Remove redundant data by inspection
This step could have been done in the beginning.This is more to remove specific variables causing troubles down the road.
```{r}
redundant_columns = c('PATNO', 'EVENT_ID', 'YEAR', 'visit_date', 'age_at_visit', 'abeta_LLOD' ,'abeta_ULOD', 'ptau_LLOD', 'tau_LLOD', 'CSFSAA','quip'
) 
PPMI.filterInspection.filterPDHealthy.filterBL = 
  PPMI.filterPDHealthy.filterBL %>% 
  select(!all_of(redundant_columns))
```

#Convert "." factor values to NA or empty
NA values in categorical variables come with there own value ".". These were changed to N/A to fit the rest of the empty values.
```{r}
PPMI.filterInspection.filterPDHealthy.filterBL = 
PPMI.filterInspection.filterPDHealthy.filterBL %>% 
  mutate(across(where(is.factor), ~ fct_recode(., NULL = "."))) 
```

#Missingness
PLot to see missing data.
```{r}

aggr_plot <- aggr(PPMI.filterInspection.filterPDHealthy.filterBL, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"), main = "Missingness Plots")
```

#Check Percent of missing data
```{r}

PPMI.filterInspection.filterPDHealthy.filterBL %>%
  summarise_all(list(name = ~sum(is.na(.))/length(.)))
```

### Drop columns with greater than 50% missing data  ###
#Drop rows with missing data among remaining columns
```{r}
PPMI.dropMissingCol= 
PPMI.filterInspection.filterPDHealthy.filterBL %>% 
  select_if(~mean(is.na(.)) < 0.50)
PPMI.dropMissingRows.Col = 
PPMI.dropMissingCol %>% 
  drop_na()
```

#Missingness after 
```{r}

aggr_plot.after <- aggr(PPMI.dropMissingRows.Col, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"), main = "Missingness Plots")

```

#Drop factor levels
Check factor levels again. Some have only 1 level. 
```{r}
PPMI.dropMissingRows.Col = 
droplevels(PPMI.dropMissingRows.Col)
```
#Drop factor columns with only one level
```{r}
columnsWithOneFactor = names(PPMI.dropMissingRows.Col[, sapply(PPMI.dropMissingRows.Col, function(col) nlevels(col) == 1)])
PPMI.dropMissingRows.Col = 
  PPMI.dropMissingRows.Col %>% 
  select(!all_of(columnsWithOneFactor))
```

#Drop levels again to make sure
```{r}
PPMI.dropMissingRows.Col = 
droplevels(PPMI.dropMissingRows.Col)

```


###################################################################
##### Feature Selection #####				 
###################################################################

Check multicolinearity for logistic regression and find and drop highest columns. 


#Build correlation matrix excluding response "CONCOHORT"
Cutoff set to 0.8. the findCorrelation function automatically decides which variable needs to be dropped. The correlated column is a series of dummy variables
```{r}
cor.matrix = cor(model.matrix(~0+., data=PPMI.dropMissingRows.Col[-1]),  use="pairwise.complete.obs")
correlatedColumn = findCorrelation(cor.matrix, cutoff = 0.8, names = TRUE)
```

#Convert dummy variable back for columns names with high colinearity
Hacky way of converting dummy variables back to correct column names to be dropped
```{r}
correlatedcolumns.todrop = gsub("[0âˆ’9]*$","",correlatedColumn)
correlatedcolumns.todrop
```

#Drop collerated columns
```{r}
PPMI.dropCollinear.dropMissingRows.Col = 
  PPMI.dropMissingRows.Col %>% 
  select(!all_of(correlatedcolumns.todrop))
```

#Correlation Plots after fixing multicolinearity
```{r}
#Take only the numeric variables
PPMI.numeric.dropCollinear.dropMissingRows.Col =  select_if(PPMI.dropCollinear.dropMissingRows.Col, is.numeric)

# Compute correlation matrix
correlations <- cor(PPMI.numeric.dropCollinear.dropMissingRows.Col,
	  use = "pairwise.complete.obs")

# Make the correlation plot
corrplot(correlations,
	type = "upper", order = "hclust",
	col = rev(brewer.pal(n = 8, name = "RdYlBu")))
```

#Find columns with little to no variance
```{r}
library(rempsyc)

#this will store the df to see output metrics, cant be used to subset
nzvtable =
  nearZeroVar(PPMI.dropCollinear.dropMissingRows.Col, saveMetrics = TRUE)
nzv.output = nzvtable[nzvtable$nzv, , drop = FALSE]
nzv.output.rownames <- tibble::rownames_to_column(nzv.output, "Variables")
nzv.output.rownames
print.table = nice_table(nzv.output.rownames)
#print(print.table, preview = "docx")
```

#Drop columns with near zero variance
Recreate nzv columns to subset the data
```{r}
#Reintrouduce nzv without metrics df for subsetting
nzv = nearZeroVar(PPMI.dropCollinear.dropMissingRows.Col)
PPMI.dropNZV.dropCollinear.dropMissingRows.Col=  PPMI.dropCollinear.dropMissingRows.Col[, -nzv]
```

###################################################################
##### Modeling Process #####				 
###################################################################

#Splitting the data in training and Testing after feature selection
```{r}
set.seed(123)
trainIndex = createDataPartition(PPMI.dropNZV.dropCollinear.dropMissingRows.Col$CONCOHORT, p = .7, list = FALSE)
training = PPMI.dropNZV.dropCollinear.dropMissingRows.Col[trainIndex, ]
testing = PPMI.dropNZV.dropCollinear.dropMissingRows.Col[-trainIndex, ]
```

#Healthy and PD breakdown 
```{r}
table_HealthyPD = table(PPMI.dropNZV.dropCollinear.dropMissingRows.Col$CONCOHORT)
table_HealthyPD
Before = round(prop.table(table_HealthyPD), digits = 2)
Before
```
#SMOTE on the training data 
```{r}
SMOTE.traindata = smote(CONCOHORT ~ ., data = training, perc.over = 2, k = 5, perc.under = 2)
```

#Healthy and PD breakdown afer SMOTE 
```{r}
table_HealthyPD.smote = table(SMOTE.traindata$CONCOHORT)
table_HealthyPD.smote
After = round(prop.table(table_HealthyPD.smote), digits = 2)
After
```

#Combine before and after table imbalance
```{r}
SMOTE.output = rbind(Before,After)
SMOTE.output.df = data.frame(SMOTE.output)
SMOTE.output.df
# colnames(SMOTE.output)
# row.names(SMOTE.output)
```
#Print and format for APA
```{r}
SMOTE.output.rownames <- tibble::rownames_to_column(SMOTE.output.df, "SMOTE")
SMOTE.output.rownames
print.SMOTE.table = nice_table(SMOTE.output.rownames)
#(print.SMOTE.table, preview = "docx")
```
###################################################################
##### Fitting the models #####				 
###################################################################

#Caret input parameters
```{r}
objControl <- trainControl(method = "cv", 
                           number = 5,
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE,
                           savePredictions = TRUE
                          )
```


#Logistic Regression 
```{r}
set.seed(123)
default.glm = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "glmnet",
  family = "binomial",
  metric = "ROC"
  
)
```

#LDA
```{r}
set.seed(123)
default.lda = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "lda",
  metric = "ROC"
  
)
```

#Random Forest
```{r}
set.seed(123)
default.rf = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "rf",
  metric = "ROC"
  
)
```

#SVM
```{r}
set.seed(123)
default.svm = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "svmRadial",
  metric = "ROC"
  
)
```


#KNN
```{r}
set.seed(123)
default.KNN = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "knn",
  metric = "ROC"
  
)
```

#XGBOOST
```{r}
set.seed(123)
default.xgboost = train(
  form = CONCOHORT ~ .,
  data = SMOTE.traindata,
  trControl = objControl,
  preProcess = c("center", "scale"),
  method = "xgbTree",
  metric = "ROC",
  verbose = FALSE,
  verbosity = 0
  
)
```

#Models
```{r}
default.glm
default.lda
default.rf
default.svm
default.KNN
default.xgboost
```
###################################################################
##### Results #####				 
###################################################################

#Checking function prediction on test data
Predicting testset data using training models. Function applies 'predict' on all algorithims.
```{r}
# model.names = c("glm", "lda", "rf", "svm", "knn", "xgboost")
models = list(glm = default.glm, lda =  default.lda,rf = default.rf, svm = default.svm, knn = default.KNN, xgboost = default.xgboost)
predictFunc = function(models, testdata, response){
  confuseMatrList = list()
  for (ii in 1:length(models)){
    test.predictions = predict(models[[ii]], testdata)
    model.name = names(models[ii])
    CM.predict = confusionMatrix(test.predictions, testdata[[response]])
    confuseMatrList <- append(confuseMatrList, lst(!!model.name :=  CM.predict)) #getting literal variables name in confuseMatrList 
  }
  return(confuseMatrList)
}

confusionMatrixOutput = predictFunc(models,testing, 'CONCOHORT' )
confusionMatrixOutput

###function representation of whats going on ###

# glm.predict = predict(default.glm, testing.noFS)
# confusionMatrix(glm.predict, testing$CONCOHORT)
# 
# lda.predict = predict(default.lda, testing.noFS)
# confusionMatrix(lda.predict, testing$CONCOHORT)
# 
# rf.predict = predict(default.rf, testing.noFS)
# confusionMatrix(rf.predict, testing$CONCOHORT)
# 
# ada.predict = predict(default.ada, testing.noFS)
# confusionMatrix(ada.predict, testing$CONCOHORT)
# 
# svm.predict = predict(default.svm, testing.noFS)
# confusionMatrix(svm.predict, testing$CONCOHORT)
# 
# knn.predict = predict(default.KNN, testing.noFS)
# confusionMatrix(knn.predict, testing$CONCOHORT)
# 
# xgboost.predict.noFS = predict(default.xgboost, testing.noFS)
# confusionMatrix(xgboost.predict.noFS, testing.noFS$CONCOHORT)
```
#Test predictions with probabilities

Testset predictions seperated by each model for plotting
```{r}
predictions.glm <- predict(default.glm, newdata=testing, type="prob")[,"PD"]
predictions.lda <- predict(default.lda, newdata=testing, type="prob")[,"PD"]
predictions.rf <- predict(default.rf, newdata=testing, type="prob")[,"PD"]
predictions.svm <- predict(default.svm, newdata=testing, type="prob")[,"PD"]
predictions.knn <- predict(default.KNN, newdata=testing, type="prob")[,"PD"]
predictions.xgboost <- predict(default.xgboost, newdata=testing, type="prob")[,"PD"]
```

#Roc curve values
Roc values from predictions. direction tells which class is represented. In this case, PD is the primary class for the roc curves. If the direction is flipped, the graph flips as well.
```{r}
roc.glm <- roc(testing$CONCOHORT, predictions.glm, levels = c("PD","Healthy"), direction = ">")
roc.lda <- roc(testing$CONCOHORT, predictions.lda,levels = c("PD","Healthy"), direction = ">")
roc.rf <- roc(testing$CONCOHORT, predictions.rf,levels = c("PD","Healthy"), direction = ">")
roc.svm <- roc(testing$CONCOHORT, predictions.svm,levels = c("PD","Healthy"), direction = ">")
roc.knn <- roc(testing$CONCOHORT, predictions.knn,levels = c("PD","Healthy"), direction = ">")
roc.xgboost <- roc(testing$CONCOHORT, predictions.xgboost,levels = c("PD","Healthy"), direction = ">")

```
#legend AUC values
```{r}
legend.glm =  sprintf("glm (AUC: %.2f)", auc(roc.glm))
legend.lda =  sprintf("lda (AUC: %.2f)", auc(roc.lda))
legend.rf =  sprintf("rf (AUC: %.2f)", auc(roc.rf))
legend.svm =  sprintf("svm (AUC: %.2f)", auc(roc.svm))
legend.knn =  sprintf("knn (AUC: %.2f)", auc(roc.knn))
legend.xgboost =  sprintf("xgboost (AUC: %.2f)", auc(roc.xgboost))

```


###### Graphs ######

#ROC comparison graph
```{r}
par(pty = "s")
plot(roc.glm, col = 'green') 
lines(roc.lda, col = 'brown') 
lines(roc.rf, col = 'yellow') 
lines(roc.svm, col = 'blue') 
lines(roc.knn, col = 'magenta') 
lines(roc.xgboost, col = 'orange') 
  legend("bottomright", col=c("green",'brown','yellow',"blue",'magenta','orange'),
         legend=c(legend.glm, legend.lda,legend.rf,legend.svm, legend.knn, legend.xgboost), lty=1, cex = 0.8)

```


#Shapley Test on random forest
Finding important predictors through approximate shapley values. 
```{r}
set.seed(123)

###stackexchange code example###
#https://stackoverflow.com/questions/72995345/approximated-shap-values-for-multi-classification-problem-using-randomforest

library(fastshap)
# A function for accessing prediction to a caret model 
p_function_PD<- function(object, newdata) 
  caret::predict.train(object, 
                       newdata = newdata, 
                       type = "prob")[,"PD"] # select PD class

shap_values_PD <- fastshap::explain(default.rf, 
                                   X = SMOTE.traindata, 
                                   pred_wrapper = p_function_PD, 
                                   nsim = 50,
                                   shap_only = FALSE,                                 newdata=SMOTE.traindata[which(SMOTE.traindata$CONCOHORT=="PD"),])
                  # select examples corresponding to category PD from 
                  # the trainset used for building the model (not shown)
                                   
```
#Plotting Shapley 
```{r}
library(shapviz)
varImp(default.rf)
shv = shapviz(shap_values_PD)
sv_importance(shv) + jtools::theme_apa()
#sv_importance(shv, kind = "bee")
```
#Making table of AUC and accuracy values from models
```{r}
AUC = list(auc(roc.glm),auc(roc.lda),auc(roc.rf), auc(roc.svm), auc(roc.knn), auc(roc.xgboost))
Accuracy = list()
Sensitivity = list()
Specificity = list()
F1 = list()
Models = as.list(names(confusionMatrixOutput))

for (ii in confusionMatrixOutput ){
  Accuracy = append(Accuracy, ii[['overall']][['Accuracy']])
  Sensitivity = append(Sensitivity, ii[["byClass"]][["Sensitivity"]])
  Specificity = append(Specificity, ii[["byClass"]][["Specificity"]])
  F1 = append(F1, ii[["byClass"]][["F1"]])
}

```

#rounding
```{r}
digits = 2
AUC = lapply(AUC, round, digits)
Accuracy = lapply(Accuracy, round, digits)
Sensitivity = lapply(Sensitivity, round, digits)
Specificity = lapply(Specificity, round, digits)
F1 = lapply(F1, round, digits)
```

#list of lists for dataframe
```{r}
listoflists = list(Models = Models,
                   Accuracy = Accuracy,
                   Sensitivity = Sensitivity,
                   Specificity = Specificity,
                   F1 = F1,
                   AUC = AUC)
Metric.dataframe = as.data.frame(do.call(cbind, listoflists))
Metric.dataframe = unnest(Metric.dataframe, cols = c(Models, Accuracy, Sensitivity, Specificity, F1, AUC))
Metric.dataframe.orderedbyAccuracy= 
Metric.dataframe %>% 
  arrange(desc(Accuracy))
```
#APA formated table of all key model metrics
```{r}
print.Metric.table = nice_table(Metric.dataframe.orderedbyAccuracy)
#print(print.Metric.table, preview = "docx")
```

#Looking at relationships
```{r}
#PPMI.dropMissingRows.Col %>% 
#gf_point(urate~ ips_caudate, color = ~CONCOHORT ) 
```
#Boxplot of top 2 predictors
```{r}
PPMI.boxplot.dropMissingRows.Col = 
PPMI.dropMissingRows.Col %>% 
  mutate(fampd_bin = fct_recode(fampd_bin, 
                                Family.History = "1",
                                No.History = "2"))
PPMI.boxplot.dropMissingRows.Col %>% 
gf_boxplot(ips_caudate ~fampd_bin, fill = ~CONCOHORT) +jtools::theme_apa()

```



###################################################################
##### Appendix A: Alternate Feature Selection #####				 
###################################################################

###Automated Feature Selection with Boruta###
For future same process of training and testing is repeated as above for boruta package. 

#Feature selection optional 
```{r}
# Perform Boruta
boruta_output <- Boruta(CONCOHORT ~ ., data=(PPMI.dropCollinear.dropMissingRows.Col),maxRuns = 300, doTrace=1)

boruta_allSelected = getSelectedAttributes(boruta_output, withTentative = TRUE)
boruta_allSelected
```

```{r}
# Plot variable importance

plot(boruta_output, cex.axis=.5, las=2, xlab="", main="Variable Importance") 
```

#Selecting boruta variables as column predictors 
```{r}
FinalPredictors = c('CONCOHORT', boruta_allSelected)
PPMI.boruta = PPMI.dropCollinear.dropMissingRows.Col[,FinalPredictors]
```